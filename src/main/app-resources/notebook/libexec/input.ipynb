{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ewf-ext-02-03-07 - NDVI anomalies of growing season per parcel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI anomalies of growing season per parcel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'NDVI anomalies of growing season per parcel'),\n",
    "                ('abstract', 'NDVI anomalies of growing season per parcel'),\n",
    "                ('id', 'ewf-ext-02-03-07')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON ((-8.864205 38.88616500000001, -8.864205 38.986165, -8.964205000000002 38.986165, -8.964205000000002 38.88616500000001, -8.864205 38.88616500000001))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "                     ('value', 'P001'),\n",
    "                     ('title', 'Name of Region'),\n",
    "                     ('abstract', 'Name of the region of interest'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexAndApiKeys = dict([('id', 'indexAndApiKeys'),\n",
    "                        ('value', ''),\n",
    "                        ('title', 'index,apikey pairs'),\n",
    "                        ('abstract', 'index,apikey pairs'),\n",
    "                        ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the NDVI stats' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_identifiers = ('97AE147C8995EA5FDA87B6342C3E6C20C41172D8', 'D0D7F08C8BFAEC14C11ACA6C824AB487764A48F8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the MODIS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/better-ext-02-03-02/search?format=atom&uid=97AE147C8995EA5FDA87B6342C3E6C20C41172D8','https://catalog.terradue.com/better-ext-02-03-06/search?format=atom&uid=D0D7F08C8BFAEC14C11ACA6C824AB487764A48F8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aux folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "#import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import copy\n",
    "\n",
    "import cioppy\n",
    "ciop = cioppy.Cioppy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata from catalog\n",
    "def get_input_metadata (input_refs, apikeys):\n",
    "    \n",
    "    # for each product get metadata\n",
    "    Result_Prod = []\n",
    "    \n",
    "    for index,product_ref in enumerate(input_refs):\n",
    "        \n",
    "        for index in apikeys:\n",
    "            if index in product_ref:\n",
    "                cat_index = index\n",
    "                cat_apikey = apikeys[index]\n",
    "        \n",
    "        # since the search is by identifier\n",
    "        Result_Prod.append(ciop.search(end_point = product_ref,params =[],output_fields='self,identifier,startdate,enclosure,startdate,enddate,wkt,title',creds='{}:{}'.format(cat_index,cat_apikey))[0] )\n",
    "    \n",
    "\n",
    "    input_metadata = gpd.GeoDataFrame.from_dict(Result_Prod)\n",
    "\n",
    "    input_metadata['startdate'] = pd.to_datetime(input_metadata['startdate'])\n",
    "    input_metadata['enddate'] = pd.to_datetime(input_metadata['enddate'])\n",
    "    \n",
    "    return input_metadata\n",
    "\n",
    "\n",
    "def get_formatted_date(datetime_str):\n",
    "    date = datetime.datetime.strftime(datetime_str, '%Y-%m-%dT%H:%M:%SZ')\n",
    " \n",
    "    return date\n",
    "\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "\n",
    "    first_date_str = get_formatted_date(first_date)\n",
    "    last_date_str = get_formatted_date(last_date)\n",
    "\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date_str, last_date_str))\n",
    "        file.write('geometry=%s' % (region_of_interest))\n",
    "        \n",
    "        \n",
    "def get_anom_dates_from_circ (agg, LTA, first_year_agg, no_data_value):\n",
    "\n",
    "    if agg == no_data_value or LTA == no_data_value:\n",
    "        return no_data_value\n",
    "    \n",
    "    aug_1st = datetime.date(first_year_agg, 8, 1).timetuple().tm_yday # 1st aug\n",
    "    ndays_year = datetime.date(first_year_agg, 12, 31).timetuple().tm_yday ## days of the first year\n",
    "\n",
    "    if agg < aug_1st and LTA < aug_1st:\n",
    "    \n",
    "        anom = agg - LTA\n",
    "    \n",
    "    elif agg >= aug_1st and LTA >= aug_1st:\n",
    "    \n",
    "        anom = agg - LTA\n",
    "    \n",
    "    elif agg < aug_1st and LTA >= aug_1st:\n",
    "    \n",
    "        d1 = ndays_year - LTA\n",
    "        d2 = agg\n",
    "        anom = d1 + d2\n",
    "    \n",
    "    elif agg >= aug_1st and LTA < aug_1st:\n",
    "    \n",
    "        d1 = ndays_year - agg\n",
    "        d2 = LTA\n",
    "        anom = -(d1 + d2)\n",
    "    \n",
    "    return anom\n",
    "\n",
    "def compute_anom_circ (agg_pds, lta_pds, first_year_agg, no_data_value):\n",
    "    \n",
    "    anoms_list = []\n",
    "    index_list = []\n",
    "    for (index_agg, value_agg), (index_lta, value_lta) in zip(agg_pds.items(), lta_pds.items()):\n",
    "    \n",
    "        index_list.append(index_agg)\n",
    "        anoms_list.append(get_anom_dates_from_circ (value_agg, value_lta, first_year_agg, no_data_value))\n",
    "    \n",
    "    return pd.Series(anoms_list, index=index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data (agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize indexes and apikeys in a python dictionary\n",
    "indexAndApiKeys_splited = indexAndApiKeys['value'].split(',')\n",
    "apikeys = {}\n",
    "for idx,ele in enumerate(indexAndApiKeys_splited):\n",
    "    \n",
    "    if (idx % 2 == 0):\n",
    "        print(ele)\n",
    "        apikeys[ele] = indexAndApiKeys_splited[idx+1]\n",
    "        \n",
    "\n",
    "# get input data from catalog\n",
    "input_metadata = get_input_metadata (input_references, apikeys)\n",
    "\n",
    "\n",
    "\n",
    "input_metadata_LTA = input_metadata[input_metadata['title'].str.find('LTA') != -1]\n",
    "input_metadata_Agg = input_metadata[input_metadata['title'].str.find('LTA') == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do start and end years belong to the same year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year_agg = input_metadata_Agg['startdate'].iloc[0].year\n",
    "end_year_agg = input_metadata_Agg['enddate'].iloc[0].year\n",
    "\n",
    "same_year = True\n",
    "if start_year_agg != end_year_agg:\n",
    "    same_year = False\n",
    "    \n",
    "same_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file paths\n",
    "filepath_agg = os.path.join(data_path, input_metadata_Agg['enclosure'].iloc[0].split('/')[-1])\n",
    "filepath_LTA = os.path.join(data_path, input_metadata_LTA['enclosure'].iloc[0].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['start_growing_season', 'end_growing_season', 'smooth_ndvi', 'dif_ndvi', 'cumulative_ndvi', 'peak_ndvi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a python dictionary\n",
    "# key -> variable name\n",
    "# content -> list of pandas dataframe, one per season (TS)\n",
    "data_agg = {}\n",
    "\n",
    "f = filepath_agg\n",
    "for var in var_names:\n",
    "\n",
    "    df = pd.read_excel (f, sheet_name=var)\n",
    "    \n",
    "    # remove useless column\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "        \n",
    "         \n",
    "    if 'start_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_avg'])\n",
    "\n",
    "        \n",
    "    if 'start_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_mode'])\n",
    "\n",
    "            \n",
    "    if 'end_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_avg'])\n",
    "        \n",
    "    if 'end_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_mode'])\n",
    "        \n",
    "    \n",
    "    data_agg[var] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data (LTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a python dictionary\n",
    "# key -> variable name\n",
    "# content -> list of pandas dataframe, one per season (TS)\n",
    "data_LTA = {}\n",
    "\n",
    "f = filepath_LTA\n",
    "for var in var_names:\n",
    "\n",
    "    df = pd.read_excel (f, sheet_name=var)\n",
    "    \n",
    "    # remove useless column\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "        \n",
    "    if 'start_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_avg'])\n",
    "\n",
    "        \n",
    "    if 'start_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_mode'])\n",
    "\n",
    "            \n",
    "    if 'end_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_avg'])\n",
    "        \n",
    "    if 'end_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_mode'])\n",
    "\n",
    "    \n",
    "    data_LTA[var] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Amomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new python dictionary to store LTAs\n",
    "anom_data = {}\n",
    "\n",
    "anom_data = copy.deepcopy(data_agg)\n",
    "\n",
    "# to each var computes mean\n",
    "for var in var_names:\n",
    "    \n",
    "    cnames = data_agg[var].columns\n",
    "    \n",
    "    for c in cnames:\n",
    "        \n",
    "        if not('start_date' in c) and not('end_date' in c):\n",
    "            \n",
    "            if 'season' in c:\n",
    "                \n",
    "                if same_year:\n",
    "                    anom_data[var][c] = data_agg[var][c] - data_LTA[var][c]\n",
    "                else:\n",
    "                    anom_data[var][c] = compute_anom_circ (data_agg[var][c], data_LTA[var][c], start_year_agg, -9999)\n",
    "            else:\n",
    "                anom_data[var][c] = data_agg[var][c].div(data_LTA[var][c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_parts = filepath_agg.split('/')[-1].split('.')[0].split('_')\n",
    "\n",
    "mission = name_parts[0]\n",
    "prod = name_parts[1]\n",
    "aoi_name = name_parts[2]\n",
    "start_date = name_parts[3]\n",
    "end_date = name_parts[4]\n",
    "\n",
    "name_parts = filepath_LTA.split('/')[-1].split('.')[0].split('_')\n",
    "\n",
    "start_year_LTA = name_parts[4]\n",
    "end_year_LTA = name_parts[5]\n",
    "\n",
    "excel_output_name = '_'.join(['Anomaly', mission, prod, aoi_name, start_date, end_date, 'LTA', start_year_LTA, end_year_LTA]) + '.xlsx'\n",
    "    \n",
    "excel_output_name = os.path.join(output_folder, excel_output_name)\n",
    "\n",
    "print(excel_output_name)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(excel_output_name) as writer:  # doctest: +SKIP\n",
    "    \n",
    "    for key in var_names:\n",
    "    \n",
    "        anom_data[key].to_excel(writer, sheet_name=key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write properties file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_properties_file(excel_output_name, input_metadata_Agg['startdate'].iloc[0], input_metadata_Agg['enddate'].iloc[0], regionOfInterest['value'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
