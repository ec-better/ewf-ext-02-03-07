{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ewf-ext-02-03-07 - NDVI anomalies of growing season per parcel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI anomalies of growing season per parcel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'NDVI anomalies of growing season per parcel'),\n",
    "                ('abstract', 'NDVI anomalies of growing season per parcel'),\n",
    "                ('id', 'ewf-ext-02-03-07')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON ((-8.864205 38.88616500000001, -8.864205 38.986165, -8.964205000000002 38.986165, -8.964205000000002 38.88616500000001, -8.864205 38.88616500000001))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "                     ('value', 'P001'),\n",
    "                     ('title', 'Name of Region'),\n",
    "                     ('abstract', 'Name of the region of interest'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexAndApiKeys = dict([('id', 'indexAndApiKeys'),\n",
    "                        ('value', ''),\n",
    "                        ('title', 'index,apikey pairs'),\n",
    "                        ('abstract', 'index,apikey pairs'),\n",
    "                        ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "This is the NDVI stats' identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_identifiers = ('E58456AA001B0A163CE829A579EFE1F4D10D2580','D4127BC272572368F1A819BCBCC312DB734FFE19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "This is the MODIS stack catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/better-ext-02-03-02/search?format=atom&uid=E58456AA001B0A163CE829A579EFE1F4D10D2580','https://catalog.terradue.com/better-ext-02-03-06/search?format=atom&uid=D4127BC272572368F1A819BCBCC312DB734FFE19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aux folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import datetime\n",
    "\n",
    "#import pdb\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import copy\n",
    "\n",
    "import cioppy\n",
    "ciop = cioppy.Cioppy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata from catalog\n",
    "def get_input_metadata (input_refs, apikeys):\n",
    "    \n",
    "    # for each product get metadata\n",
    "    Result_Prod = []\n",
    "    \n",
    "    for index,product_ref in enumerate(input_refs):\n",
    "        \n",
    "        for index in apikeys:\n",
    "            if index in product_ref:\n",
    "                cat_index = index\n",
    "                cat_apikey = apikeys[index]\n",
    "        \n",
    "        # since the search is by identifier\n",
    "        Result_Prod.append(ciop.search(end_point = product_ref,params =[],output_fields='self,identifier,startdate,enclosure,startdate,enddate,wkt,title',creds='{}:{}'.format(cat_index,cat_apikey))[0] )\n",
    "    \n",
    "\n",
    "    input_metadata = gpd.GeoDataFrame.from_dict(Result_Prod)\n",
    "\n",
    "    input_metadata['startdate'] = pd.to_datetime(input_metadata['startdate'])\n",
    "    input_metadata['enddate'] = pd.to_datetime(input_metadata['enddate'])\n",
    "    \n",
    "    return input_metadata\n",
    "\n",
    "\n",
    "def get_formatted_date(datetime_str):\n",
    "    date = datetime.datetime.strftime(datetime_str, '%Y-%m-%dT%H:%M:%SZ')\n",
    " \n",
    "    return date\n",
    "\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "\n",
    "    first_date_str = get_formatted_date(first_date)\n",
    "    last_date_str = get_formatted_date(last_date)\n",
    "\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date_str, last_date_str))\n",
    "        file.write('geometry=%s' % (region_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data (agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize indexes and apikeys in a python dictionary\n",
    "indexAndApiKeys_splited = indexAndApiKeys['value'].split(',')\n",
    "apikeys = {}\n",
    "for idx,ele in enumerate(indexAndApiKeys_splited):\n",
    "    \n",
    "    if (idx % 2 == 0):\n",
    "        print(ele)\n",
    "        apikeys[ele] = indexAndApiKeys_splited[idx+1]\n",
    "        \n",
    "\n",
    "# get input data from catalog\n",
    "input_metadata = get_input_metadata (input_references, apikeys)\n",
    "\n",
    "\n",
    "\n",
    "input_metadata_LTA = input_metadata[input_metadata['title'].str.find('LTA') != -1]\n",
    "input_metadata_Agg = input_metadata[input_metadata['title'].str.find('LTA') == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file paths\n",
    "filepath_agg = os.path.join(data_path, input_metadata_Agg['enclosure'].iloc[0].split('/')[-1])\n",
    "filepath_LTA = os.path.join(data_path, input_metadata_LTA['enclosure'].iloc[0].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['start_growing_season', 'end_growing_season', 'smooth_ndvi', 'dif_ndvi', 'cumulative_ndvi', 'peak_ndvi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a python dictionary\n",
    "# key -> variable name\n",
    "# content -> list of pandas dataframe, one per season (TS)\n",
    "data_agg = {}\n",
    "\n",
    "f = filepath_agg\n",
    "for var in var_names:\n",
    "\n",
    "    df = pd.read_excel (f, sheet_name=var)\n",
    "    \n",
    "    # remove useless column\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "        \n",
    "         \n",
    "    if 'start_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_avg'])\n",
    "\n",
    "        \n",
    "    if 'start_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_mode'])\n",
    "\n",
    "            \n",
    "    if 'end_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_avg'])\n",
    "        \n",
    "    if 'end_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_mode'])\n",
    "        \n",
    "    \n",
    "    data_agg[var] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data (LTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a python dictionary\n",
    "# key -> variable name\n",
    "# content -> list of pandas dataframe, one per season (TS)\n",
    "data_LTA = {}\n",
    "\n",
    "f = filepath_LTA\n",
    "for var in var_names:\n",
    "\n",
    "    df = pd.read_excel (f, sheet_name=var)\n",
    "    \n",
    "    # remove useless column\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns=['Unnamed: 0'])\n",
    "        \n",
    "    if 'start_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_avg'])\n",
    "\n",
    "        \n",
    "    if 'start_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['start_growing_season_date_mode'])\n",
    "\n",
    "            \n",
    "    if 'end_growing_season_date_avg' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_avg'])\n",
    "        \n",
    "    if 'end_growing_season_date_mode' in df.columns:\n",
    "\n",
    "        # drop date\n",
    "        df = df.drop(columns=['end_growing_season_date_mode'])\n",
    "\n",
    "    \n",
    "    data_LTA[var] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Amomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new python dictionary to store LTAs\n",
    "anom_data = {}\n",
    "\n",
    "anom_data = copy.deepcopy(data_agg)\n",
    "\n",
    "# to each var computes mean\n",
    "for var in var_names:\n",
    "    \n",
    "    cnames = data_agg[var].columns\n",
    "    \n",
    "    for c in cnames:\n",
    "        \n",
    "        if not('start_date' in c) and not('end_date' in c):\n",
    "            \n",
    "            if 'season' in c:\n",
    "                anom_data[var][c] = data_agg[var][c] - data_LTA[var][c]\n",
    "            else:\n",
    "                anom_data[var][c] = data_agg[var][c].div(data_LTA[var][c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_parts = filepath_agg.split('/')[-1].split('.')[0].split('_')\n",
    "\n",
    "mission = name_parts[0]\n",
    "prod = name_parts[1]\n",
    "aoi_name = name_parts[2]\n",
    "start_date = name_parts[3]\n",
    "end_date = name_parts[4]\n",
    "\n",
    "name_parts = filepath_LTA.split('/')[-1].split('.')[0].split('_')\n",
    "\n",
    "start_year_LTA = name_parts[4]\n",
    "end_year_LTA = name_parts[5]\n",
    "\n",
    "excel_output_name = '_'.join(['Anomaly', mission, prod, aoi_name, start_date, end_date, 'LTA', start_year_LTA, end_year_LTA]) + '.xlsx'\n",
    "    \n",
    "excel_output_name = os.path.join(output_folder, excel_output_name)\n",
    "\n",
    "print(excel_output_name)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(excel_output_name) as writer:  # doctest: +SKIP\n",
    "    \n",
    "    for key in var_names:\n",
    "    \n",
    "        anom_data[key].to_excel(writer, sheet_name=key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write properties file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_properties_file(excel_output_name, input_metadata_Agg['startdate'].iloc[0], input_metadata_Agg['enddate'].iloc[0], regionOfInterest['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
